{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3d396ca-a510-4a14-900b-76a1d0fbf855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. What is an activation function in the context of artificial neural networks?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#An activation function in the context of artificial neural networks is a mathematical function that introduces non-linearity to the output of a neuron. It determines whether a neuron should be activated (fire) or not based on the weighted sum of its inputs. Activation functions are a crucial component of neural networks, as they allow the network to learn complex relationships and make them capable of solving non-linear problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea1e8fb4-1d46-494d-998d-5272f2df9826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. What are some common types of activation functions used in neural networks?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Some common types of activation functions used in neural networks are:\n",
    "\n",
    "#Sigmoid function (logistic)\n",
    "#Rectified Linear Unit (ReLU)\n",
    "#Leaky ReLU\n",
    "#Parametric ReLU (PReLU)\n",
    "#Exponential Linear Unit (ELU)\n",
    "#Hyperbolic Tangent (tanh)\n",
    "#Softmax (mainly used in the output layer for multi-class classification problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6efff5fe-13b4-4fc9-b69f-4c21c7f79650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. How do activation functions affect the training process and performance of a neural network?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Activation functions greatly impact the training process and performance of a neural network. They introduce non-linearity, which allows the network to learn complex patterns and relationships in the data. The choice of activation function can influence convergence speed, model expressiveness, and the risk of vanishing or exploding gradients during backpropagation. Choosing an appropriate activation function is crucial to avoid issues like vanishing gradients and enable more efficient and accurate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced86fa8-1ae3-4f3c-b308-396d62f7bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. How does the sigmoid activation function work? What are its advantages and disadvantages?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#The sigmoid activation function maps the input to a range between 0 and 1. It is defined as f(x) = 1 / (1 + exp(-x)). Sigmoid was widely used in the past but is less common now due to some disadvantages:\n",
    "\n",
    "#Advantages:\n",
    "\n",
    "#It squashes the input to a probability-like value, suitable for binary classification problems.\n",
    "#It is differentiable, making it usable with gradient-based optimization methods.\n",
    "\n",
    "#Disadvantages:\n",
    "    \n",
    "#Sigmoid suffers from the vanishing gradient problem, which can slow down or hinder training in deep networks.\n",
    "#It outputs non-zero centered values, which can cause issues during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f36b21a2-de53-42d2-b4c7-ae5a43c6062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#The Rectified Linear Unit (ReLU) activation function is defined as f(x) = max(0, x). It returns 0 for any negative input and passes the positive input as is. ReLU differs from the sigmoid function in several ways:\n",
    "\n",
    "#ReLU is not bounded, and its range is from 0 to positive infinity, whereas sigmoid ranges from 0 to 1.\n",
    "#ReLU is computationally more efficient since it involves a simple threshold operation, while sigmoid involves exponentiation.\n",
    "#ReLU helps alleviate the vanishing gradient problem, as it does not saturate for positive values, promoting faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d07baa3-826f-4e5a-855f-f4e89c5859fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. What are the benefits of using the ReLU activation function over the sigmoid function?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#The benefits of using the ReLU activation function over sigmoid include:\n",
    "\n",
    "#Faster convergence during training due to non-saturation and avoidance of vanishing gradient problems for positive inputs.\n",
    "#Computational efficiency, as ReLU involves a simple threshold operation without exponentiation, making it faster to compute.\n",
    "#It allows the network to learn sparse representations, as it sets negative values to zero, resulting in fewer active neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c206c33-599b-44aa-bfc1-20738a818ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem.\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Leaky ReLU is a modification of the ReLU activation function that addresses the vanishing gradient problem for negative inputs. It is defined as f(x) = max(α * x, x), where α is a small positive slope for negative inputs (usually a small constant like 0.01). The introduction of the slope allows the neurons to have non-zero outputs even for negative inputs, preventing the vanishing gradient problem and enabling learning in deeper networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "047c38de-e9d9-4a91-89df-d3518b46e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. What is the purpose of the softmax activation function? When is it commonly used?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#The softmax activation function is used in the output layer of a neural network for multi-class classification problems. It converts the raw scores (logits) of each class into a probability distribution, where the sum of probabilities of all classes adds up to 1. It is commonly used when the neural network needs to classify inputs into one of several exclusive classes (e.g., image classification, sentiment analysis, language translation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "777a248f-6afd-44bb-8def-9d944c706348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#The hyperbolic tangent (tanh) activation function is defined as f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x)). It is similar to the sigmoid function but maps the input to a range between -1 and 1. Like the sigmoid, it is also susceptible to the vanishing gradient problem for extreme values. However, unlike the sigmoid, the tanh function is zero-centered, which can make optimization easier in certain cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b14676-43a4-4c67-924f-740c4fb6c2c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
